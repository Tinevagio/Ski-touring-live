"""
Scraper Camptocamp API v6 pour r√©cup√©rer des itin√©raires ski de rando
Format de sortie compatible avec itineraires_alpes.csv
"""

import requests
import pandas as pd
import time
from typing import List, Dict, Optional
import re

# Configuration
BASE_URL = "https://api.camptocamp.org/routes"
OUTPUT_FILE = "data/raw/itineraires_alpes_camptocamp.csv"
MAX_ROUTES = 500  # Nombre d'itin√©raires √† r√©cup√©rer

# Mapping des massifs depuis les areas Camptocamp
# UNIQUEMENT ALPES FRAN√áAISES - correspond aux massifs BERA
MASSIF_MAPPING = {
    # Haute-Savoie
    "chablais": "Chablais",
    "aravis": "Aravis",
    "mont-blanc": "Mont-Blanc",
    "chamonix": "Mont-Blanc",
    
    # Savoie
    "bauges": "Bauges",
    "beaufortain": "Beaufortain",
    "haute-tarentaise": "Haute-Tarentaise",
    "tarentaise": "Haute-Tarentaise",
    "maurienne": "Maurienne",
    "vanoise": "Vanoise",
    "haute-maurienne": "Haute-Maurienne",
    
    # Is√®re
    "chartreuse": "Chartreuse",
    "belledonne": "Belledonne",
    "grandes-rousses": "Grandes-Rousses",
    "rousses": "Grandes-Rousses",
    "vercors": "Vercors",
    "oisans": "Oisans",
    
    # Hautes-Alpes / Alpes du Sud
    "thabor": "Thabor",
    "pelvoux": "Pelvoux",
    "ecrins": "Pelvoux",
    "queyras": "Queyras",
    "devoluy": "Devoluy",
    "d√©voluy": "Devoluy",
    "champsaur": "Champsaur",
    "embrunais": "Embrunais-Parpaillon",
    "parpaillon": "Embrunais-Parpaillon",
    
    # Alpes-de-Haute-Provence
    "ubaye": "Ubaye",
    "haut-var": "Haut-Var Haut-Verdon",
    "haut-verdon": "Haut-Var Haut-Verdon",
    "verdon": "Haut-Var Haut-Verdon",
    
    # Alpes-Maritimes
    "mercantour": "Mercantour",
}

# Mapping cotations ski Camptocamp ‚Üí format S1-S5
DIFFICULTY_MAPPING = {
    "S1": "S1", "S2": "S2", "S3": "S3", "S4": "S4", "S5": "S5",
    "1.1": "S1", "1.2": "S1", "1.3": "S1",
    "2.1": "S2", "2.2": "S2", "2.3": "S2",
    "3.1": "S3", "3.2": "S3", "3.3": "S3",
    "4.1": "S4", "4.2": "S4", "4.3": "S4",
    "5.1": "S5", "5.2": "S5", "5.3": "S5",
}

# Mapping expositions
EXPO_MAPPING = {
    "N": "N", "NE": "NE", "E": "E", "SE": "SE",
    "S": "S", "SW": "SO", "W": "O", "NW": "NO",
    "n": "N", "ne": "NE", "e": "E", "se": "SE",
    "s": "S", "sw": "SO", "w": "O", "nw": "NO",
}


def fetch_routes_batch(offset: int = 0, limit: int = 100) -> Optional[Dict]:
    """R√©cup√®re un batch de routes depuis l'API Camptocamp"""
    params = {
        "act": "skitouring",  # Filtre ski de rando
        "offset": offset,
        "limit": limit,
        "pl": "fr",  # Langue fran√ßaise prioritaire
        # Pas de bbox - on filtre c√¥t√© client
    }
    
    headers = {
        "User-Agent": "SkiTouringLive/1.0 (Educational Project)",
        "Accept": "application/json"
    }
    
    try:
        response = requests.get(BASE_URL, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur requ√™te API (offset {offset}): {e}")
        return None


def parse_massif(areas: List[Dict]) -> str:
    """Extrait le massif depuis les areas - filtre Alpes fran√ßaises uniquement"""
    if not areas:
        return None  # Pas de massif = on skip
    
    # Parcours des areas pour trouver un match connu (massifs fran√ßais)
    for area in areas:
        area_name = area.get("area_type", "")
        if area_name == "range":  # C'est un massif/range
            name = area.get("locales", [{}])[0].get("title", "").lower()
            for key, value in MASSIF_MAPPING.items():
                if key in name:
                    return value
    
    # Si aucun match dans les massifs fran√ßais connus, on skip cette route
    return None


def parse_coordinates(geometry: Dict) -> tuple:
    """Parse les coordonn√©es lat/lon depuis geometry"""
    if not geometry or "geom" not in geometry:
        return None, None
    
    geom = geometry["geom"]
    
    # Format: {"type":"Point","coordinates":[x,y]} ou string JSON
    if isinstance(geom, str):
        # Parse string JSON
        match = re.search(r'"coordinates":\s*\[([\d.-]+),\s*([\d.-]+)', geom)
        if match:
            x, y = float(match.group(1)), float(match.group(2))
            
            # Camptocamp utilise des coordonn√©es en m√®tres (EPSG:3857 Web Mercator)
            # Il faut les convertir en lat/lon (EPSG:4326 WGS84)
            # Formule approximative pour les Alpes
            if abs(x) > 1000:  # Si > 1000, c'est des m√®tres, pas des degr√©s
                # Conversion Web Mercator ‚Üí WGS84
                from math import pi, atan, exp
                lon = x / 20037508.34 * 180
                lat = atan(exp(y / 20037508.34 * pi)) * 360 / pi - 90
                return lat, lon
            else:
                # D√©j√† en lat/lon
                return y, x  # Attention: Camptocamp fait [lon, lat]
            
    elif isinstance(geom, dict) and "coordinates" in geom:
        coords = geom["coordinates"]
        if len(coords) >= 2:
            x, y = coords[0], coords[1]
            
            # M√™me logique
            if abs(x) > 1000:
                from math import pi, atan, exp
                lon = x / 20037508.34 * 180
                lat = atan(exp(y / 20037508.34 * pi)) * 360 / pi - 90
                return lat, lon
            else:
                return y, x  # [lon, lat] ‚Üí (lat, lon)
    
    return None, None


def parse_exposition(doc: Dict) -> str:
    """Extrait l'exposition dominante"""
    # Champ orientations (liste des faces)
    orientations = doc.get("orientations", [])
    if orientations:
        first_orient = orientations[0].upper()
        return EXPO_MAPPING.get(first_orient, first_orient)
    
    # Champ ski_exposition
    ski_expo = doc.get("ski_exposition", "")
    if ski_expo and len(ski_expo) >= 1:
        return EXPO_MAPPING.get(ski_expo[0].upper(), "N")
    
    return "N"  # D√©faut : Nord


def parse_difficulty(doc: Dict) -> str:
    """Extrait la difficult√© ski"""
    # Cotation ski labande
    labande = doc.get("labande_ski_rating", "")
    if labande:
        return DIFFICULTY_MAPPING.get(labande, "S3")
    
    # Cotation ski classique
    ski_rating = doc.get("ski_rating", "")
    if ski_rating:
        return DIFFICULTY_MAPPING.get(ski_rating, "S3")
    
    # Global rating (1-5)
    global_rating = doc.get("global_rating", "")
    if global_rating and global_rating.isdigit():
        rating_num = int(global_rating)
        if 1 <= rating_num <= 5:
            return f"S{rating_num}"
    
    return "S3"  # D√©faut : niveau interm√©diaire


def parse_route(doc: Dict) -> Optional[Dict]:
    """Parse un document route en format CSV attendu"""
    # Nom (fran√ßais prioritaire)
    locales = doc.get("locales", [])
    if not locales:
        return None
    
    name = locales[0].get("title", "Unknown")
    if not name or name == "Unknown":
        return None
    
    # Coordonn√©es
    geometry = doc.get("geometry", {})
    lat, lon = parse_coordinates(geometry)
    if lat is None or lon is None:
        return None  # Skip si pas de coordonn√©es valides
    
    # Filtre g√©ographique : Alpes fran√ßaises uniquement
    # Lat: 44.0-47.5, Lon: 5.0-8.0
    if not (44.0 <= lat <= 47.5 and 5.0 <= lon <= 8.0):
        return None  # Hors zone Alpes fran√ßaises
    
    # D√©nivel√©
    denivele = doc.get("height_diff_up")
    if not denivele or denivele == 0:
        return None  # Skip si pas de D+ (probablement incomplet)
    
    try:
        denivele = int(denivele)
    except (ValueError, TypeError):
        return None  # Skip si d√©nivel√© invalide
    
    # Massif
    areas = doc.get("areas", [])
    massif = parse_massif(areas)
    
    # Skip si pas un massif fran√ßais connu
    if massif is None:
        # Debug: affiche pourquoi on skip (d√©commenter pour debug)
        # area_names = [a.get("locales", [{}])[0].get("title", "?") for a in areas]
        # print(f"      ‚ö†Ô∏è Skip '{name[:30]}' - massif non reconnu: {area_names}")
        return None
    
    # Exposition
    exposition = parse_exposition(doc)
    
    # Difficult√©
    difficulty = parse_difficulty(doc)
    
    return {
        "name": name,
        "massif": massif,
        "lat": round(lat, 4),
        "lon": round(lon, 4),
        "denivele_positif": denivele,
        "exposition": exposition,
        "difficulty_ski": difficulty
    }


def fetch_all_routes(max_routes: int = 500) -> List[Dict]:
    """R√©cup√®re tous les itin√©raires jusqu'√† max_routes"""
    all_routes = []
    offset = 0
    batch_size = 100
    
    print(f"üéø R√©cup√©ration de {max_routes} itin√©raires ALPES FRAN√áAISES depuis Camptocamp...")
    print(f"üìç Zone : Latitude 44.0-47.5, Longitude 5.0-8.0\n")
    
    while len(all_routes) < max_routes:
        print(f"üì° Requ√™te offset={offset}...", end=" ")
        
        data = fetch_routes_batch(offset=offset, limit=batch_size)
        if not data:
            print("‚ùå √âchec")
            break
        
        documents = data.get("documents", [])
        total_available = data.get("total", 0)
        
        if not documents:
            print("‚úÖ Plus de routes disponibles")
            break
        
        print(f"‚úÖ {len(documents)} routes r√©cup√©r√©es (Total dispo: {total_available})")
        
        # Parse chaque route
        parsed_count = 0
        skip_reasons = {"no_coords": 0, "out_of_zone": 0, "no_denivele": 0, "no_massif": 0}
        
        for doc in documents:
            parsed = parse_route(doc)
            if parsed:
                all_routes.append(parsed)
                parsed_count += 1
            else:
                # Compte les raisons de skip pour debug
                if not doc.get("geometry"):
                    skip_reasons["no_coords"] += 1
                elif doc.get("geometry"):
                    lat, lon = parse_coordinates(doc.get("geometry", {}))
                    if lat and not (44.0 <= lat <= 47.5 and 5.0 <= lon <= 8.0):
                        skip_reasons["out_of_zone"] += 1
                    elif not doc.get("height_diff_up"):
                        skip_reasons["no_denivele"] += 1
                    else:
                        skip_reasons["no_massif"] += 1
        
        reasons_str = ", ".join([f"{k}: {v}" for k, v in skip_reasons.items() if v > 0])
        print(f"   ‚Üí {parsed_count} routes valides | Skip: {reasons_str} (Total: {len(all_routes)})")
        
        offset += batch_size
        time.sleep(0.5)  # Rate limiting poli
        
        # Stop si on a atteint le max
        if len(all_routes) >= max_routes:
            break
    
    return all_routes[:max_routes]


def main():
    """Point d'entr√©e principal"""
    print("=" * 60)
    print("üèîÔ∏è  CAMPTOCAMP ROUTE SCRAPER - ALPES FRAN√áAISES UNIQUEMENT")
    print("=" * 60)
    print("Massifs BERA couverts : Chablais, Aravis, Mont-Blanc, Bauges,")
    print("Beaufortain, Vanoise, Chartreuse, Belledonne, Maurienne,")
    print("Vercors, Oisans, Pelvoux, Queyras, Mercantour, etc.\n")
    
    # R√©cup√©ration
    routes = fetch_all_routes(max_routes=MAX_ROUTES)
    
    if not routes:
        print("\n‚ùå Aucune route r√©cup√©r√©e. V√©rifie ton acc√®s internet et l'API Camptocamp.")
        return
    
    # Conversion en DataFrame
    df = pd.DataFrame(routes)
    
    # D√©doublonnage par nom
    df_unique = df.drop_duplicates(subset=["name"], keep="first")
    duplicates_removed = len(df) - len(df_unique)
    
    print(f"\nüìä Statistiques:")
    print(f"   ‚Ä¢ Routes r√©cup√©r√©es: {len(routes)}")
    print(f"   ‚Ä¢ Doublons supprim√©s: {duplicates_removed}")
    print(f"   ‚Ä¢ Routes uniques: {len(df_unique)}")
    print(f"\nüìç R√©partition par massif:")
    print(df_unique["massif"].value_counts().head(10))
    print(f"\n‚õ∑Ô∏è  R√©partition par difficult√©:")
    print(df_unique["difficulty_ski"].value_counts())
    
    # Sauvegarde
    df_unique.to_csv(OUTPUT_FILE, index=False, encoding="utf-8")
    print(f"\n‚úÖ Fichier sauvegard√©: {OUTPUT_FILE}")
    print(f"\nüéâ Termin√© ! Tu peux maintenant utiliser ce CSV dans ton app.")
    
    # Preview
    print("\nüìã Aper√ßu des 5 premi√®res routes:")
    print(df_unique.head().to_string(index=False))


if __name__ == "__main__":
    main()